# -*- coding: utf-8 -*-
"""Unsupervised.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14-pxUnrhenmI3FsIToTGJJneR5JpXeza
"""

"""
Retail Analytics: Customer Segmentation + Market Basket (Apriori)
- End-to-end notebook-friendly workflow with plots and BI summary.
"""

# ============================== Imports =======================================
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import (
    silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score
)

from scipy.cluster.hierarchy import dendrogram, linkage

from mlxtend.frequent_patterns import apriori, association_rules

# Optional (for association-rule network)
try:
    import networkx as nx
    HAS_NX = True
except Exception:
    HAS_NX = False

from pandas.plotting import parallel_coordinates


# ======================= Data Loading & Exploration ===========================
df = pd.read_csv("synthetic_retail_transactions.csv", parse_dates=["InvoiceDate"])
print("Rows:", len(df))
df.head()

print("\nDataset Overview:")
print(df.info())

print("Column Descriptions:")
print("- InvoiceNo: Invoice number (unique per transaction)")
print("- CustomerID: Unique customer identifier")
print("- InvoiceDate: Date and time of transaction")
print("- ProductID: Product code")
print("- ProductName: Product name")
print("- Category: Product category")
print("- UnitPrice: Price per unit")
print("- Quantity: Number of items purchased")
print("- Amount: Total price (Quantity × UnitPrice)")

# --- FIX #1: ensure Amount exists ---
if "Amount" not in df.columns:
    df["Amount"] = df["Quantity"] * df["UnitPrice"]

print("\nBasic Statistics:")
print(f"Total transaction rows: {len(df)}")
print(f"Unique transactions (InvoiceNo): {df['InvoiceNo'].nunique()}")
print(f"Unique customers: {df['CustomerID'].nunique()}")
print(f"Unique products: {df['ProductID'].nunique()}")
print(f"Date range: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}")

# ================= Data Preprocessing and Feature Engineering =================
# Remove invalid lines
df = df[df["Quantity"] > 0]
df = df[df["UnitPrice"] > 0]

print("Count of Missing Values\n")
missing_summary = df.isnull().sum()
missing_percent = (df.isnull().sum() / len(df)) * 100
missing_report = pd.DataFrame({
    "MissingValues": missing_summary,
    "PercentMissing": missing_percent.round(2)
})
print(missing_report)

# ================================= RFM Analysis ===============================
current_date = df["InvoiceDate"].max() + pd.Timedelta(days=1)

rfm = df.groupby("CustomerID").agg({
    "InvoiceDate": lambda x: (current_date - x.max()).days,   # Recency
    "InvoiceNo": "nunique",                                    # Frequency
    "Amount": "sum"                                            # Monetary
}).reset_index()
rfm.columns = ["CustomerID", "Recency", "Frequency", "Monetary"]

print("\nRFM Table Summary:")
print(rfm.describe())
print("\nSample RFM data:")
rfm.head(10)

# -------- Add additional behavioral features --------
avg_order_value = (
    df.groupby("CustomerID")["Amount"].mean().reset_index(name="AvgOrderValue")
)
product_diversity = (
    df.groupby("CustomerID")["ProductID"].nunique().reset_index(name="ProductDiversity")
)
total_qty = (
    df.groupby("CustomerID")["Quantity"].sum().reset_index(name="TotalQuantity")
)
avg_qty_per_txn = (
    df.groupby("CustomerID")["Quantity"].mean().reset_index(name="AvgQuantityPerTransaction")
)
category_diversity = (
    df.groupby("CustomerID")["Category"].nunique().reset_index(name="CategoryDiversity")
)
purchase_span = (
    df.groupby("CustomerID")["InvoiceDate"]
      .agg(lambda x: (x.max() - x.min()).days)
      .reset_index(name="PurchaseSpan")
)
avg_days_between = (
    df.groupby("CustomerID")["InvoiceDate"]
      .apply(lambda x: (x.sort_values().diff().mean().days) if len(x) > 1 else np.nan)
      .reset_index(name="AvgDaysBetweenPurchases")
)

rfm = (
    rfm.merge(avg_order_value, on="CustomerID")
       .merge(product_diversity, on="CustomerID")
       .merge(total_qty, on="CustomerID")
       .merge(avg_qty_per_txn, on="CustomerID")
       .merge(category_diversity, on="CustomerID")
       .merge(purchase_span, on="CustomerID")
       .merge(avg_days_between, on="CustomerID")
)

print(f"RFM table shape: {rfm.shape[0]} rows × {rfm.shape[1]} columns")
print("\nFirst 10 rows of RFM table with extra features:\n")
print(rfm.head(10).to_string(index=False))

print("\nRFM Table Columns:")
for col in rfm.columns:
    print(f"- {col}")

# ======================= Visualizations for RFM Metrics =======================
sns.set(style="whitegrid")

feature_titles = {
    "Recency": "Days Since Last Purchase",
    "Frequency": "Number of Purchases",
    "Monetary": "Total Amount Spent ($)",
    "AvgOrderValue": "Average Spend Per Order ($)",
    "ProductDiversity": "Number of Different Products Bought",
    "TotalQuantity": "Total Items Bought",
    "PurchaseSpan": "Days Between First and Last Purchase",
    "AvgDaysBetweenPurchases": "Average Days Between Purchases"
}
features_to_plot = list(feature_titles.keys())

fig, axes = plt.subplots(3, 3, figsize=(18, 12))
axes = axes.flatten()
for i, feature in enumerate(features_to_plot):
    sns.histplot(rfm[feature], bins=40, kde=False, color='steelblue', edgecolor='black', ax=axes[i])
    axes[i].set_title(feature_titles[feature])
    axes[i].set_xlabel(feature_titles[feature])
    axes[i].set_ylabel("Number of Customers")
for j in range(len(features_to_plot), len(axes)):
    fig.delaxes(axes[j])
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(data=rfm[["Recency", "Frequency", "Monetary"]], palette="pastel")
plt.title("Boxplots of RFM Metrics")
plt.ylabel("Value")
plt.show()

plt.figure(figsize=(8, 6))
sns.scatterplot(x="Recency", y="Monetary", data=rfm, hue="Frequency",
                palette="viridis", size="Frequency", sizes=(20, 200))
plt.title("Recency vs Monetary (Color = Frequency)")
plt.xlabel("Recency (days since last purchase)")
plt.ylabel("Total Spending (Monetary)")
plt.legend(title="Frequency", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# =============================== Feature Scaling =============================
clustering_features = [
    "Recency", "Frequency", "Monetary", "AvgOrderValue", "ProductDiversity",
    "TotalQuantity", "AvgQuantityPerTransaction", "CategoryDiversity",
    "PurchaseSpan", "AvgDaysBetweenPurchases"
]
X = rfm[clustering_features].copy()
X.replace([np.inf, -np.inf], np.nan, inplace=True)
X = X.fillna(X.median(numeric_only=True))

Q1 = X.quantile(0.25)
Q3 = X.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print("Removing outliers using IQR...")
mask = ~((X < lower_bound) | (X > upper_bound)).any(axis=1)
X_clean = X[mask].copy()
rfm_clean = rfm.loc[mask].copy()
print(f"Original data: {len(X)} customers")
print(f"After outlier removal: {len(X_clean)} customers")
print(f"Outliers removed: {len(X) - len(X_clean)} "
      f"({(len(X) - len(X_clean)) / len(X) * 100:.1f}%)")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_clean)
print("\nFeatures have been standardized (mean=0, std=1).")

# ============================== Find Optimal K ================================
X_mat = X_scaled
inertia = []
silhouette_scores_list = []
K_range = range(2, 10)
for k in K_range:
    km = KMeans(n_clusters=k, n_init=20, random_state=42)
    labels = km.fit_predict(X_mat)
    inertia.append(km.inertia_)
    sil = silhouette_score(X_mat, labels) if len(np.unique(labels)) > 1 else np.nan
    silhouette_scores_list.append(sil)

plt.figure(figsize=(6, 4))
plt.plot(list(K_range), inertia, marker="o")
plt.xlabel("K (number of clusters)")
plt.ylabel("Inertia")
plt.title("Elbow Method")
plt.show()

def top_k_by_silhouette(X_scaled, k_min=2, k_max=10, top_n=3, n_init=20, random_state=42, plot=True):
    Ks = list(range(k_min, k_max + 1))
    sils = []
    for k in Ks:
        km = KMeans(n_clusters=k, n_init=n_init, random_state=random_state)
        labels = km.fit_predict(X_scaled)
        sil = silhouette_score(X_scaled, labels) if len(np.unique(labels)) > 1 else np.nan
        sils.append(sil)
    results = pd.DataFrame({"K": Ks, "Silhouette": sils})
    results_sorted = results.sort_values("Silhouette", ascending=False).reset_index(drop=True)
    top_k_values = results_sorted.head(top_n)["K"].tolist()
    if plot:
        plt.figure(figsize=(6, 4))
        plt.plot(results["K"], results["Silhouette"], marker="o")
        plt.xlabel("K (number of clusters)")
        plt.ylabel("Silhouette Score")
        plt.title("Silhouette vs K")
        for k_val in top_k_values:
            sil_val = results.loc[results["K"] == k_val, "Silhouette"].values[0]
            plt.scatter(k_val, sil_val, color='red', s=100, zorder=5)
        plt.show()
    return results_sorted, top_k_values

results_df, top_k_list = top_k_by_silhouette(X_scaled, k_min=2, k_max=10, top_n=3, plot=True)
print("Ranked K values by silhouette score:")
print(results_df)
print(f"Top {len(top_k_list)} K values:", top_k_list)

# ============================ Apply K-Means ===================================
optimal_k = 5
kmeans = KMeans(n_clusters=optimal_k, init="k-means++", max_iter=300, n_init=20, random_state=42)
clusters = kmeans.fit_predict(X_scaled)
rfm_clean = rfm_clean.copy()
rfm_clean["Cluster"] = clusters

cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
cluster_centers_df = pd.DataFrame(cluster_centers, columns=clustering_features)
cluster_centers_df["Cluster"] = range(optimal_k)
print("Cluster Centers (Original Scale):")
print(cluster_centers_df.round(2).to_string(index=False))

cluster_sizes = rfm_clean["Cluster"].value_counts().sort_index()
print("\nCluster Sizes (filtered customers only):")
for cluster, size in cluster_sizes.items():
    pct = size / len(rfm_clean) * 100
    print(f"Cluster {cluster}: {size} customers ({pct:.1f}%)")

# ============================ Visualize Clusters ==============================
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(10, 8))
sc = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6, s=25)
plt.xlabel(f'First Principal Component (explains {pca.explained_variance_ratio_[0]:.1%} variance)')
plt.ylabel(f'Second Principal Component (explains {pca.explained_variance_ratio_[1]:.1%} variance)')
plt.title('Customer Segments Visualization (2D PCA)')
plt.colorbar(sc, label='Cluster')
plt.grid(True, alpha=0.3)

centroids_pca = pca.transform(kmeans.cluster_centers_)
plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1],
            marker='X', s=200, c='black', edgecolors='white', linewidths=1.5, label='Centroid')
for i, (x, y) in enumerate(centroids_pca):
    plt.text(x, y, f" {i}", va="center", ha="left", fontsize=10, weight="bold", color="black")
plt.legend(loc='upper right', frameon=True)
plt.tight_layout()
plt.show()

print(f"Total variance explained by 2 components: {pca.explained_variance_ratio_.sum():.1%}")

# ========================= Analyze Cluster Characteristics ====================
cluster_profiles = rfm_clean.groupby('Cluster')[clustering_features].mean().round(2)
print("Average Values by Cluster:")
print(cluster_profiles)

from math import pi
cluster_profiles_normalized = (cluster_profiles - cluster_profiles.min()) / (cluster_profiles.max() - cluster_profiles.min())
categories = clustering_features
fig = plt.figure(figsize=(12, 8))
num_vars = len(categories)
angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]
angles += angles[:1]
ax = plt.subplot(111, polar=True)
plt.xticks(angles[:-1], categories)
colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']
for idx, row in cluster_profiles_normalized.iterrows():
    values = row.values.flatten().tolist()
    values += values[:1]
    ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {idx}', color=colors[idx % len(colors)])
    ax.fill(angles, values, alpha=0.25, color=colors[idx % len(colors)])
plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))
plt.title('Cluster Profiles Comparison', size=16, y=1.08)
plt.show()

# ============================== Customer Personas =============================
def create_persona(cluster_row):
    r = cluster_row["Recency"]
    f = cluster_row["Frequency"]
    m = cluster_row["Monetary"]
    aov = cluster_row["AvgOrderValue"]
    prod_div = cluster_row["ProductDiversity"]
    cat_div = cluster_row["CategoryDiversity"]
    qty_total = cluster_row["TotalQuantity"]
    qty_per_txn = cluster_row["AvgQuantityPerTransaction"]
    span = cluster_row["PurchaseSpan"]
    gap = cluster_row["AvgDaysBetweenPurchases"]

    if (r <= 45) and (f >= 6) and (span >= 240) and (prod_div >= 9 or cat_div >= 6):
        return ("Loyal Frequent Shoppers",
                "Very engaged customers who buy often, purchase a wide range of items, and have a long relationship.")
    if (m >= 90) and (qty_total >= 25) and (qty_per_txn >= 2.5):
        return ("Bulk High-Spend Buyers",
                "Big baskets and higher spend; likely stocking up. Great candidates for bundles and subscriptions.")
    if (r <= 90) and (3 <= f <= 6) and (span >= 200):
        return ("Steady Long-Term Buyers",
                "Reliable, moderate spenders who return over many months. Maintain with loyalty perks and light promos.")
    if (r >= 100) and (3 <= f <= 6) and (prod_div >= 7 or cat_div >= 5):
        return ("Variety Seekers (Lapsed)",
                "Not active recently but sample many items when they shop. Reactivate with try-something-new offers.")
    if (f <= 2) and (m < 35) and (span <= 90):
        return ("One-Off Low-Spend Buyers",
                "Low engagement and low spend. Use low-cost win-back or focus budget elsewhere unless behavior changes.")
    return ("Emerging/Potential Loyalists",
            "Showing promise; nurture with personalized recommendations and loyalty nudges.")

personas = {}
for cluster_id, row in cluster_profiles.iterrows():
    name, desc = create_persona(row)
    personas[cluster_id] = {
        "name": name,
        "description": desc,
        "size": int(cluster_sizes.get(cluster_id, 0)),
        "metrics": {k: float(v) for k, v in row.to_dict().items()}
    }

print("CUSTOMER PERSONAS BY CLUSTER")
print("=" * 50)
for cid, p in personas.items():
    print(f"\nCluster {cid}: {p['name']}")
    print(f"Description: {p['description']}")
    print(f"Size: {p['size']} customers")
    print("Key Metrics (cluster means):")
    for k, v in p["metrics"].items():
        print(f"  - {k}: {v:.2f}")

# ======================== Alternate: Hierarchical Clustering ==================
sample_indices = np.random.choice(len(X_scaled), size=min(100, len(X_scaled)), replace=False)
X_sample = X_scaled[sample_indices]
linkage_matrix = linkage(X_sample, method='ward')
plt.figure(figsize=(12, 6))
dendrogram(linkage_matrix, truncate_mode='level', p=6)
plt.title('Hierarchical Clustering Dendrogram (Sample)')
plt.xlabel('Customer Index'); plt.ylabel('Distance')
plt.show()

agg_clustering = AgglomerativeClustering(n_clusters=optimal_k)
agg_clusters = agg_clustering.fit_predict(X_scaled)

similarity = adjusted_rand_score(clusters, agg_clusters)
print(f"\nSimilarity between K-Means and Hierarchical Clustering: {similarity:.2f}")
print("(1.0 = identical, 0.0 = random)")

# =========================== Gaussian Mixture Model ===========================
gmm = GaussianMixture(n_components=optimal_k, covariance_type='full', random_state=42)
gmm_labels = gmm.fit_predict(X_scaled)
gmm_silhouette = silhouette_score(X_scaled, gmm_labels)
print(f"Gaussian Mixture silhouette (k={optimal_k}): {gmm_silhouette:.3f}")

ari_kmeans = adjusted_rand_score(clusters, gmm_labels)
ari_agglo = adjusted_rand_score(agg_clusters, gmm_labels)
print(f"Similarity GMM vs KMeans (ARI): {ari_kmeans:.3f}")
print(f"Similarity GMM vs Agglomerative (ARI): {ari_agglo:.3f}")

gmm_cluster_means = pd.DataFrame(X_clean, columns=clustering_features)
gmm_cluster_means['Cluster_GMM'] = gmm_labels
gmm_means = gmm_cluster_means.groupby('Cluster_GMM')[clustering_features].mean().round(1)
print("\nGaussian Mixture Cluster Averages (key metrics):")
print(gmm_means[['Recency', 'Frequency', 'Monetary']])
print("\nQuick Persona-style Summary (GMM):")
for c, row in gmm_means[['Recency', 'Frequency', 'Monetary']].iterrows():
    print(f"  Cluster {c}: Recency={row['Recency']}, Freq={row['Frequency']}, Spend={row['Monetary']}")

# ===================== Compare 3 Methods (sizes & monetary) ===================
methods = {
    "K-Means": np.asarray(clusters),
    "Agglomerative": np.asarray(agg_clusters),
    "GMM": np.asarray(gmm_labels),
}
summary = {}
for name, labs in methods.items():
    sizes_c = pd.Series(labs).value_counts().sort_index()
    tmp = rfm_clean.copy()
    tmp["_label"] = labs
    means_c = tmp.groupby("_label")["Monetary"].mean().sort_index()
    summary[name] = {"sizes": sizes_c, "monetary_means": means_c}

fig, axes = plt.subplots(2, 3, figsize=(15, 8))
fig.suptitle("Comparison of Customer Segmentation Methods", fontsize=14)
method_names = list(methods.keys())

for j, name in enumerate(method_names):
    ax = axes[0, j]
    sizes_c = summary[name]["sizes"]
    bars = ax.bar(sizes_c.index.astype(str), sizes_c.values)
    ax.set_title(f"{name}: Cluster Sizes")
    ax.set_xlabel("Cluster"); ax.set_ylabel("Customers")
    ax.grid(axis="y", alpha=0.3)
    for b in bars:
        ax.text(b.get_x() + b.get_width()/2, b.get_height(), f"{int(b.get_height())}",
                ha="center", va="bottom", fontsize=9)

for j, name in enumerate(method_names):
    ax = axes[1, j]
    means_c = summary[name]["monetary_means"].round(1)
    bars = ax.bar(means_c.index.astype(str), means_c.values)
    ax.set_title(f"{name}: Average Spend per Customer ($)")
    ax.set_xlabel("Cluster"); ax.set_ylabel("Average Monetary ($)")
    ax.grid(axis="y", alpha=0.3)
    for b, val in zip(bars, means_c.values):
        ax.text(b.get_x() + b.get_width()/2, b.get_height(), f"${val:.0f}",
                ha="center", va="bottom", fontsize=9)

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

# ===================== Evaluation & Ranking of Methods ========================
def evaluate_segmentations(X_scaled, rfm_clean, k=5, random_state=42,
                           weights=None, return_labels=False):
    if weights is None:
        weights = {'sil': 0.4, 'ch': 0.2, 'db': 0.2, 'mon': 0.2}

    km = KMeans(n_clusters=k, n_init=20, random_state=random_state)
    km_labels = km.fit_predict(X_scaled)

    agg = AgglomerativeClustering(n_clusters=k, linkage="ward", metric="euclidean")
    agg_labels = agg.fit_predict(X_scaled)

    gmm = GaussianMixture(n_components=k, covariance_type="full", random_state=random_state)
    gmm_labels = gmm.fit_predict(X_scaled)

    methods_local = {"K-Means": km_labels, "Agglomerative": agg_labels, "GMM": gmm_labels}

    money = rfm_clean["Monetary"].values
    def mon_sep(labels):
        means = [money[labels == c].mean() for c in np.unique(labels)]
        return float(np.max(means) - np.min(means))

    rows = []
    for name, y in methods_local.items():
        sil = silhouette_score(X_scaled, y)
        ch = calinski_harabasz_score(X_scaled, y)
        db = davies_bouldin_score(X_scaled, y)
        ms = mon_sep(y)
        rows.append([name, sil, ch, db, ms])

    df_eval = pd.DataFrame(rows, columns=["Method", "Silhouette", "CalinskiHarabasz", "DaviesBouldin", "MonetarySeparation"])

    def norm_pos(x):  return (x - x.min()) / (x.max() - x.min() + 1e-12)
    def norm_neg(x):  return (x.max() - x.min() - (x - x.min())) / (x.max() - x.min() + 1e-12)

    df_eval["sil_n"] = norm_pos(df_eval["Silhouette"])
    df_eval["ch_n"]  = norm_pos(df_eval["CalinskiHarabasz"])
    df_eval["db_n"]  = norm_neg(df_eval["DaviesBouldin"])
    df_eval["mon_n"] = norm_pos(df_eval["MonetarySeparation"])

    df_eval["FinalScore"] = (
        weights['sil'] * df_eval["sil_n"] +
        weights['ch']  * df_eval["ch_n"]  +
        weights['db']  * df_eval["db_n"]  +
        weights['mon'] * df_eval["mon_n"]
    )

    out = (df_eval[["Method", "Silhouette", "CalinskiHarabasz", "DaviesBouldin", "MonetarySeparation", "FinalScore"]]
           .sort_values("FinalScore", ascending=False)
           .reset_index(drop=True)
           .round({"Silhouette": 3, "CalinskiHarabasz": 1, "DaviesBouldin": 3, "MonetarySeparation": 2, "FinalScore": 3}))

    ari_km_agg  = adjusted_rand_score(km_labels, agg_labels)
    ari_km_gmm  = adjusted_rand_score(km_labels, gmm_labels)
    ari_agg_gmm = adjusted_rand_score(agg_labels, gmm_labels)

    print("Pairwise ARI (agreement between methods):")
    print(f"  K-Means vs Agglomerative: {ari_km_agg:.3f}")
    print(f"  K-Means vs GMM:           {ari_km_gmm:.3f}")
    print(f"  Agglomerative vs GMM:     {ari_agg_gmm:.3f}\n")

    print("Clustering Method Comparison (ranked):")
    print(out.to_string(index=False))

    if return_labels:
        return out, methods_local
    return out

results = evaluate_segmentations(X_scaled, rfm_clean, k=5, random_state=42)

# ================= Market Basket Analysis (Apriori + Explorer) ================
MIN_SUPPORT = 0.02   # rubric: 0.01–0.05
MIN_CONF    = 0.30   # rubric: 0.25–0.50
MIN_LIFT    = 1.10   # mild strength

def build_boolean_basket(df_in, product_col=None):
    if product_col is None:
        for c in ["ProductName", "Description", "ProductID"]:
            if c in df_in.columns:
                product_col = c
                break
        if product_col is None:
            raise ValueError("Expected a product column: ProductName or Description or ProductID")
    sub = df_in.loc[:, ["InvoiceNo", product_col, "Quantity"]].copy()
    sub = sub.dropna(subset=["InvoiceNo", product_col, "Quantity"])
    sub["Quantity"] = pd.to_numeric(sub["Quantity"], errors="coerce")
    sub = sub.dropna(subset=["Quantity"])
    sub = sub[sub["Quantity"] > 0]
    basket = (sub.groupby(["InvoiceNo", product_col])["Quantity"].sum().unstack(fill_value=0))
    basket_bool = basket.gt(0)
    return basket_bool, product_col

def mine_rules(basket_bool, min_support=MIN_SUPPORT, min_conf=MIN_CONF, sort_by=("lift","confidence")):
    itemsets = apriori(basket_bool, min_support=min_support, use_colnames=True)
    rules = association_rules(itemsets, metric="confidence", min_threshold=min_conf)
    rules = rules.query("support >= @min_support").copy()
    rules["antecedents"] = rules["antecedents"].apply(lambda s: list(s))
    rules["consequents"] = rules["consequents"].apply(lambda s: list(s))
    rules["antecedent_len"] = rules["antecedents"].apply(len)
    rules["consequent_len"] = rules["consequents"].apply(len)
    rules = rules.sort_values(list(sort_by), ascending=False).reset_index(drop=True)
    return rules, itemsets

def filter_rules(rules_df, include=None, exclude=None,
                 min_support=MIN_SUPPORT, min_conf=MIN_CONF, min_lift=MIN_LIFT,
                 max_antecedent_len=None, top_n=50):
    r = rules_df.copy()
    r = r[(r["support"] >= min_support) & (r["confidence"] >= min_conf) & (r["lift"] >= min_lift)]
    if max_antecedent_len is not None:
        r = r[r["antecedent_len"] <= int(max_antecedent_len)]
    if include:
        inc = set(include)
        r = r[r["antecedents"].apply(lambda ants: inc.issubset(set(ants)))]
    if exclude:
        exc = set(exclude)
        r = r[~r["antecedents"].apply(lambda ants: bool(set(ants) & exc))]
        r = r[~r["consequents"].apply(lambda cons: bool(set(cons) & exc))]
    r = r.sort_values(["lift","confidence","support"], ascending=False)
    return r.head(top_n)

def recommend_from_rules(products, rules_df, top_n=10):
    prods = set(products)
    if not prods:
        return pd.DataFrame(columns=["consequent","support","confidence","lift"])
    mask = rules_df["antecedents"].apply(lambda ants: set(ants).issubset(prods))
    recs = (rules_df[mask][["consequents","support","confidence","lift"]]
                .explode("consequents")
                .rename(columns={"consequents":"consequent"}))
    recs = recs[~recs["consequent"].isin(prods)]
    recs = recs.sort_values(["lift","confidence","support"], ascending=False).head(top_n)
    return recs.reset_index(drop=True)

def plot_rules_scatter(rules_df, title="Rules: Support vs Confidence (size = Lift)"):
    if rules_df.empty:
        print("No rules to plot."); return
    r = rules_df.copy()
    sizes = 500 * (r["lift"] - r["lift"].min()) / (r["lift"].max() - r["lift"].min() + 1e-12) + 20
    plt.figure(figsize=(7,5))
    plt.scatter(r["support"], r["confidence"], s=sizes, alpha=0.6)
    plt.xlabel("Support"); plt.ylabel("Confidence")
    plt.title(title); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()

def plot_rules_histograms(rules_df):
    if rules_df.empty:
        print("No rules to plot."); return
    fig, axes = plt.subplots(1, 3, figsize=(12,4))
    for ax, col in zip(axes, ["support","confidence","lift"]):
        ax.hist(rules_df[col].values, bins=30, edgecolor="black")
        ax.set_title(f"Distribution of {col}")
        ax.set_xlabel(col.capitalize()); ax.set_ylabel("Count")
    plt.tight_layout(); plt.show()

def draw_rules_network(rules_df, top_n=25, title="Association Rules Network (top by lift)"):
    if not HAS_NX:
        print("networkx not installed; skipping network graph."); return
    if rules_df.empty:
        print("No rules to visualize."); return
    r = rules_df.sort_values(["lift","confidence"], ascending=False).head(top_n).copy()
    G = nx.DiGraph()
    def lbl(items): return " + ".join(map(str, items))
    for _, row in r.iterrows():
        a = lbl(row["antecedents"]); c = lbl(row["consequents"]); w = float(row["lift"])
        G.add_node(a, kind="antecedent"); G.add_node(c, kind="consequent")
        G.add_edge(a, c, lift=w, support=float(row["support"]), confidence=float(row["confidence"]))
    pos = nx.spring_layout(G, k=0.8, seed=42)
    plt.figure(figsize=(10, 7))
    lifts = np.array([d["lift"] for _,_,d in G.edges(data=True)])
    if len(lifts) == 0: print("No edges to draw."); return
    widths = 1.0 + 3.0 * (lifts - lifts.min()) / (lifts.max() - lifts.min() + 1e-12)
    kinds = nx.get_node_attributes(G, "kind")
    nodes_a = [n for n,k in kinds.items() if k=="antecedent"]
    nodes_c = [n for n,k in kinds.items() if k=="consequent"]
    nx.draw_networkx_nodes(G, pos, nodelist=nodes_a, node_shape="o", node_color="#4c72b0", alpha=0.8)
    nx.draw_networkx_nodes(G, pos, nodelist=nodes_c, node_shape="s", node_color="#dd8452", alpha=0.8)
    nx.draw_networkx_edges(G, pos, width=widths, arrows=True, arrowstyle="->", alpha=0.5)
    nx.draw_networkx_labels(G, pos, font_size=9)
    plt.title(title); plt.axis("off"); plt.tight_layout(); plt.show()

def plot_rules_parallel_coords(rules_df, top_n=25, title="Rule Metrics — Parallel Coordinates"):
    if rules_df.empty:
        print("No rules to plot."); return
    cols = [c for c in ["support","confidence","lift","leverage","conviction"] if c in rules_df.columns]
    if not cols:
        print("Required metric columns not found for parallel coordinates."); return
    r = rules_df.sort_values("lift", ascending=False).head(top_n).copy()
    def make_label(row):
        a = " + ".join(map(str, row["antecedents"]))
        c = " + ".join(map(str, row["consequents"]))
        return f"{a} → {c}"
    r["rule_label"] = r.apply(make_label, axis=1)
    plot_df = r[["rule_label"] + cols].copy()
    for c in cols:
        v = plot_df[c].values.astype(float)
        lo, hi = np.nanmin(v), np.nanmax(v)
        plot_df[c] = (v - lo) / (hi - lo + 1e-12)
    plt.figure(figsize=(11, 6))
    parallel_coordinates(plot_df, "rule_label", color=None, linewidth=1.5)
    plt.title(title); plt.ylabel("Normalized metric (0–1)")
    plt.xticks(rotation=0); plt.legend([], [], frameon=False)
    plt.tight_layout(); plt.show()

# --------- Run MAB Pipeline ----------
source_df = df  # use your cleaned df; change to df_clean if desired

basket_bool, PRODUCT_COL = build_boolean_basket(source_df)
print(f"Basket built with product column: {PRODUCT_COL}")
print(f"Shape: {basket_bool.shape[0]} invoices × {basket_bool.shape[1]} products")

rules_all, frequent_itemsets = mine_rules(
    basket_bool, min_support=MIN_SUPPORT, min_conf=MIN_CONF, sort_by=("lift","confidence")
)
print(f"Frequent itemsets: {len(frequent_itemsets)} | Rules: {len(rules_all)}")

rules_view = filter_rules(
    rules_all,
    include=None,
    exclude=None,
    min_support=MIN_SUPPORT,
    min_conf=MIN_CONF,
    min_lift=MIN_LIFT,
    max_antecedent_len=3,
    top_n=25
)
cols_to_show = [c for c in ["antecedents","consequents","support","confidence","lift","leverage","conviction"]
                if c in rules_view.columns]
print("\nRules (filtered view, top by lift):")
print(rules_view[cols_to_show].head(10))

plot_rules_scatter(rules_view, title="Rules (filtered): Support vs Confidence (size = Lift)")
plot_rules_histograms(rules_view)
draw_rules_network(rules_view, top_n=20, title="Association Rules Network (top 20 by lift)")
plot_rules_parallel_coords(rules_view, top_n=20)

# Example:
# recs = recommend_from_rules({"Pasta", "Tomato Sauce"}, rules_all, top_n=10)
# print("\nRecommendations for basket {Pasta, Tomato Sauce}:")
# print(recs)

# ====================== BI Visuals + Quick ROI Projection =====================
sizes = rfm_clean["Cluster"].value_counts().sort_index()
aov_by_cluster = rfm_clean.groupby("Cluster")["AvgOrderValue"].mean().round(2)
print("Cluster sizes:\n", sizes, "\n")
print("Average order value by cluster:\n", aov_by_cluster, "\n")

def top_rule_strings(rules_df, n=5):
    r = rules_df.sort_values(["lift","confidence"], ascending=False).head(n)
    out = []
    for _, row in r.iterrows():
        a = ", ".join(map(str, row["antecedents"]))
        c = ", ".join(map(str, row["consequents"]))
        out.append(f"{a} ⇒ {c}  (support={row['support']:.2%}, conf={row['confidence']:.2%}, lift={row['lift']:.2f})")
    return out

print("Top rules to cite:")
for s in top_rule_strings(rules_view, n=5):
    print(" -", s)

MARGIN = 0.35
ELIGIBLE_BASKETS = 10000
COST = 1500
prod_prevalence = basket_bool.mean(axis=0).to_dict()

def roi_for_rules(rules_df, top_n=10):
    rows = []
    r = rules_df.sort_values(["lift","confidence"], ascending=False).head(top_n).copy()
    for _, row in r.iterrows():
        ants = list(row["antecedents"])
        cons = list(row["consequents"])
        base_attach = np.mean([prod_prevalence.get(c, 0.0) for c in cons])
        cons_prices = df[df[PRODUCT_COL].isin(cons)]["UnitPrice"]
        avg_price = cons_prices.mean() if not cons_prices.empty else rfm_clean["AvgOrderValue"].mean()
        lift = float(row["lift"])
        inc_attach = max(lift - 1, 0) * base_attach
        incr_rev = ELIGIBLE_BASKETS * inc_attach * avg_price * MARGIN
        roi = (incr_rev - COST) / COST
        rows.append({
            "antecedents": ", ".join(ants),
            "consequents": ", ".join(cons),
            "base_attach_%": round(base_attach*100, 2),
            "lift": round(lift, 2),
            "incremental_attach_%": round(inc_attach*100, 2),
            "avg_consequent_price": round(avg_price, 2),
            "eligible_baskets": ELIGIBLE_BASKETS,
            "gross_margin": MARGIN,
            "campaign_cost": COST,
            "expected_incremental_revenue": round(incr_rev, 2),
            "ROI": round(roi, 2)
        })
    return pd.DataFrame(rows)

roi_table = roi_for_rules(rules_view, top_n=10)
print("\nQuick ROI table (top rules):")
print(roi_table.to_string(index=False))

# ---------------------- BI Summary + ROI Charts ------------------------------
assert 'rfm_clean' in globals(), "rfm_clean not found. Run RFM step."
assert 'clusters'   in globals(), "clusters not found. Run K-Means."
assert 'df'         in globals(), "df not found."
assert 'PRODUCT_COL' in globals(), "PRODUCT_COL not set."

if 'rules_view' in globals() and not (rules_view is None or rules_view.empty):
    RULES_FOR_BI = rules_view.copy()
elif 'rules_all' in globals() and not rules_all.empty:
    RULES_FOR_BI = rules_all.copy()
else:
    raise ValueError("No rules available. Run Apriori first.")

if 'cluster_profiles' not in globals():
    clustering_features_soft = [c for c in rfm_clean.columns if c not in ["CustomerID","Cluster"]]
    cluster_profiles = rfm_clean.groupby("Cluster")[clustering_features_soft].mean().round(2)

sizes = rfm_clean["Cluster"].value_counts().sort_index()
aov_by_cluster = rfm_clean.groupby("Cluster")["AvgOrderValue"].mean()
chosen_method = "K-Means (k=5)"
if 'results' in globals():
    chosen_method = f"{results.iloc[0]['Method']} (k=5)"

print("=== Executive Summary (auto-filled numbers) ===")
print(f"Chosen segmentation method: {chosen_method}")
print("\nCluster sizes:")
for cid, cnt in sizes.items():
    print(f"  Cluster {cid}: {cnt} customers")
print("\nAverage Order Value by cluster ($):")
for cid, val in aov_by_cluster.round(2).items():
    print(f"  Cluster {cid}: {val:.2f}")

def short_rule_strings(rules_df, n=5):
    r = rules_df.sort_values(["lift","confidence"], ascending=False).head(n)
    out = []
    for _, row in r.iterrows():
        a = ", ".join(map(str, row["antecedents"]))
        c = ", ".join(map(str, row["consequents"]))
        out.append(f"{a} ⇒ {c}  (support={row['support']:.2%}, conf={row['confidence']:.2%}, lift={row['lift']:.2f})")
    return out

print("\nTop rules to cite (by lift):")
for s in short_rule_strings(RULES_FOR_BI, n=5):
    print(" -", s)

MARGIN = 0.35
ELIGIBLE_BASKETS = 10000
COST = 1500
prod_prevalence = basket_bool.mean(axis=0).to_dict()

def build_roi_table(rules_df, df_transactions, product_col, top_n=10,
                    eligible=ELIGIBLE_BASKETS, margin=MARGIN, cost=COST):
    rows = []
    r = rules_df.sort_values(["lift","confidence"], ascending=False).head(top_n).copy()
    for _, row in r.iterrows():
        ants = list(row["antecedents"])
        cons = list(row["consequents"])
        base_attach = float(np.mean([prod_prevalence.get(c, 0.0) for c in cons]))
        cons_prices = df_transactions[df_transactions[product_col].isin(cons)]["UnitPrice"]
        avg_price = float(cons_prices.mean()) if not cons_prices.empty else float(rfm_clean["AvgOrderValue"].mean())
        lift = float(row["lift"])
        inc_attach = max(lift - 1, 0) * base_attach
        incr_rev  = eligible * inc_attach * avg_price * margin
        roi       = (incr_rev - cost) / cost if cost else np.nan
        rows.append({
            "antecedents": ", ".join(ants),
            "consequents": ", ".join(cons),
            "support": float(row["support"]),
            "confidence": float(row["confidence"]),
            "lift": lift,
            "base_attach_%": base_attach * 100,
            "incremental_attach_%": inc_attach * 100,
            "avg_consequent_price": avg_price,
            "eligible_baskets": int(eligible),
            "gross_margin": float(margin),
            "campaign_cost": float(cost),
            "expected_incremental_revenue": incr_rev,
            "ROI": roi
        })
    return pd.DataFrame(rows)

roi_table = build_roi_table(
    RULES_FOR_BI, df_transactions=df, product_col=PRODUCT_COL, top_n=10,
    eligible=ELIGIBLE_BASKETS, margin=MARGIN, cost=COST
)

disp_cols = [
    "antecedents","consequents","support","confidence","lift",
    "base_attach_%","incremental_attach_%","avg_consequent_price",
    "eligible_baskets","gross_margin","campaign_cost",
    "expected_incremental_revenue","ROI"
]
print("\nQuick ROI table (top rules):")
print(roi_table[disp_cols].round({
    "support":3, "confidence":3, "lift":2, "base_attach_%":2, "incremental_attach_%":2,
    "avg_consequent_price":2, "expected_incremental_revenue":2, "ROI":2
}).to_string(index=False))

roi_table.to_csv("roi_rules_table.csv", index=False)
print("\nSaved: roi_rules_table.csv")

plt.figure(figsize=(6,4))
bars = plt.bar(sizes.index.astype(str), sizes.values)
plt.title("Customer Count by Cluster")
plt.xlabel("Cluster"); plt.ylabel("Customers"); plt.grid(axis="y", alpha=0.3)
for b in bars:
    plt.text(b.get_x()+b.get_width()/2, b.get_height(), f"{int(b.get_height())}",
             ha="center", va="bottom", fontsize=9)
plt.tight_layout(); plt.show()

means_monetary = rfm_clean.groupby("Cluster")["Monetary"].mean()
plt.figure(figsize=(6,4))
bars = plt.bar(means_monetary.index.astype(str), means_monetary.values)
plt.title("Average Spend per Customer by Cluster ($)")
plt.xlabel("Cluster"); plt.ylabel("Average Monetary ($)"); plt.grid(axis="y", alpha=0.3)
for b, val in zip(bars, means_monetary.values):
    plt.text(b.get_x()+b.get_width()/2, b.get_height(), f"${val:.0f}",
             ha="center", va="bottom", fontsize=9)
plt.tight_layout(); plt.show()

top_rules = RULES_FOR_BI.sort_values(["lift","confidence"], ascending=False).head(10).copy()
labels = [", ".join(map(str, a)) + " → " + ", ".join(map(str, c))
          for a, c in zip(top_rules["antecedents"], top_rules["consequents"])]
plt.figure(figsize=(10, max(4, 0.45*len(top_rules))))
plt.barh(range(len(top_rules)), top_rules["lift"].values)
plt.yticks(range(len(top_rules)), labels)
plt.gca().invert_yaxis()
plt.xlabel("Lift"); plt.title("Top Association Rules by Lift")
plt.tight_layout(); plt.show()

roi_sorted = roi_table.sort_values("expected_incremental_revenue", ascending=False).head(10)
labels_roi = [f"{a} → {c}" for a, c in zip(roi_sorted["antecedents"], roi_sorted["consequents"])]
plt.figure(figsize=(10, max(4, 0.45*len(roi_sorted))))
plt.barh(range(len(roi_sorted)), roi_sorted["expected_incremental_revenue"].values)
plt.yticks(range(len(roi_sorted)), labels_roi)
plt.gca().invert_yaxis()
plt.xlabel("Expected Incremental Revenue ($)")
plt.title(f"ROI Projection (eligible={ELIGIBLE_BASKETS:,}, margin={MARGIN:.0%}, cost=${COST:,})")
plt.tight_layout(); plt.show()

r = top_rules.copy()
sizes_bub = 500 * (r["lift"] - r["lift"].min()) / (r["lift"].max() - r["lift"].min() + 1e-12) + 20
plt.figure(figsize=(7,5))
plt.scatter(r["support"], r["confidence"], s=sizes_bub, alpha=0.6)
for i, lbl in enumerate(labels):
    plt.text(r["support"].iloc[i] + 0.001, r["confidence"].iloc[i] + 0.001, str(i+1), fontsize=8)
plt.xlabel("Support"); plt.ylabel("Confidence")
plt.title("Top Rules: Support vs Confidence (size = Lift)")
plt.grid(alpha=0.3); plt.tight_layout(); plt.show()

# ================== FIX #2: Seasonal / Temporal Patterns + Checklist ==========
# Map each invoice to a month (using earliest timestamp per invoice)
inv_month = (
    df.groupby("InvoiceNo")["InvoiceDate"].min()
      .dt.to_period("M").astype(str)
).reindex(basket_bool.index)

# 1) Monthly product prevalence for top-N products
TOP_N_PRODUCTS = 15
overall_freq = basket_bool.mean(axis=0).sort_values(ascending=False)
top_products = overall_freq.head(TOP_N_PRODUCTS).index

monthly_support = (
    basket_bool[top_products]
    .assign(__month=inv_month)
    .groupby("__month").mean()
    .sort_index()
)

plt.figure(figsize=(10, 6))
plt.imshow(monthly_support.T, aspect="auto", interpolation="nearest")
plt.colorbar(label="Support (share of baskets)")
plt.yticks(range(len(top_products)), top_products)
plt.xticks(range(len(monthly_support.index)), monthly_support.index, rotation=45, ha="right")
plt.title("Monthly Product Support (Top Products)")
plt.tight_layout()
plt.show()

# 2) Monthly confidence for top-N rules by lift
def _monthly_confidence_for_rule(antecedents, consequents):
    # conf_month(A -> C) = support_month(A ∪ C) / support_month(A)
    A_mask = basket_bool[antecedents].all(axis=1)
    AC_cols = list(set(antecedents) | set(consequents))
    AC_mask = basket_bool[AC_cols].all(axis=1)

    dfm = pd.DataFrame({"A": A_mask.values, "AC": AC_mask.values, "__month": inv_month.values})
    dfm = dfm.dropna(subset=["__month"])

    by_m = dfm.groupby("__month")
    sup_A  = by_m["A"].mean()
    sup_AC = by_m["AC"].mean()
    conf_m = (sup_AC / sup_A).replace([np.inf, -np.inf], np.nan)
    return conf_m.index.tolist(), conf_m.values

TOP_RULES_N = 8
rules_for_season = (
    (rules_view if 'rules_view' in globals() and not rules_view.empty else rules_all)
    .sort_values(["lift","confidence"], ascending=False)
    .head(TOP_RULES_N)
)

plt.figure(figsize=(10, 6))
all_months = None
for _, row in rules_for_season.iterrows():
    months, conf_vals = _monthly_confidence_for_rule(row["antecedents"], row["consequents"])
    label = f"{' + '.join(row['antecedents'])} → {' + '.join(row['consequents'])}"
    x = np.arange(len(months))
    plt.plot(x, conf_vals, marker="o", alpha=0.8, label=label)
    all_months = months

if all_months is None:
    print("No seasonal lines to plot (no rules).")
else:
    plt.xticks(np.arange(len(all_months)), all_months, rotation=45, ha="right")
    plt.ylabel("Monthly Confidence")
    plt.title("Monthly Confidence for Top Association Rules")
    plt.grid(alpha=0.3)
    plt.legend(bbox_to_anchor=(1.02, 1.0), loc="upper left", frameon=True, fontsize=8)
    plt.tight_layout()
    plt.show()

seasonal_done = True

# ----------------------- Requirements Checklist ------------------------------
def _has_cols(d, cols):
    return all(c in d.columns for c in cols)

checks = []
checks.append(("RFM features present",
               _has_cols(rfm, ["Recency","Frequency","Monetary"])))
checks.append(("Extra behavior features present",
               _has_cols(rfm, [
                   "AvgOrderValue","ProductDiversity","TotalQuantity",
                   "AvgQuantityPerTransaction","CategoryDiversity",
                   "PurchaseSpan","AvgDaysBetweenPurchases"
               ])))
checks.append(("Removed invalid quantities/prices",
               (df["Quantity"] > 0).all() and (df["UnitPrice"] > 0).all()))
checks.append(("K-Means fitted", "Cluster" in rfm_clean.columns))
checks.append(("Elbow & silhouette done",
               ("results_df" in globals() and not pd.isna(results_df["Silhouette"]).all())
               or ("results" in globals())))
checks.append(("Alternative clustering present",
               ('agg_clusters' in globals()) or ('gmm_labels' in globals())))
checks.append(("Validation metrics computed", 'results' in globals()))
checks.append(("Personas created",
               'personas' in globals() and isinstance(personas, dict) and len(personas) > 0))

checks.append(("Apriori itemsets & rules built",
               'rules_all' in globals() and isinstance(rules_all, pd.DataFrame) and not rules_all.empty))
checks.append(("Thresholds meet rubric (support/confidence)",
               (0.01 <= MIN_SUPPORT <= 0.05) and (0.25 <= MIN_CONF <= 0.50)))
checks.append(("Lift/Leverage/Conviction included",
               all(c in rules_all.columns for c in ["lift","leverage","conviction"])))
checks.append(("Rules explorer function available", 'filter_rules' in globals()))
checks.append(("Recommender function available", 'recommend_from_rules' in globals()))
checks.append(("Rules visuals available", 'plot_rules_scatter' in globals()))
checks.append(("Seasonal patterns analyzed", 'seasonal_done' in globals() and seasonal_done is True))

print("\n== REQUIREMENTS CHECKLIST ==")
for name, ok in checks:
    print(f"{name}: {'PASS' if ok else 'MISSING'}")
